{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3fbbba3",
   "metadata": {},
   "source": [
    "# Medical Image Segmentation Using ü§ó HuggingFace & PyTorch\n",
    "\n",
    "Medical image segmentation is an innovative process that enables surgeons to have a virtual \"x-ray vision.\" It is a highly valuable tool in healthcare, providing non-invasive diagnostics and in-depth analysis. With this in mind, in this post, we will explore the UW-Madison GI Tract Image Segmentation Kaggle challenge dataset. As part of this project, we will utilize PyTorch along with PyTorch-Lightning. We will use ü§ó HuggingFace transformers to load and fine-tune the Segformer transformer-based model on the medical segmentation dataset. Finally, we will create a Gradio app for image inference and deploy it on HuggingFace spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe723702",
   "metadata": {},
   "source": [
    "<img src=\"https://learnopencv.com/wp-content/uploads/2023/07/medical-image-segmentation_feature_Image.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a95edf8",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "\n",
    "* [1 Install & Import Necessary Libraries](#1-Install-&-Import-Necessary-Libraries)\n",
    "* [2 Set Hyperparameters For The Project](#2-Set-Hyperparameters-For-The-Project)\n",
    "* [3 Loading The Medical Image Segmentation Dataset](#3-Loading-The-Medical-Image-Segmentation-Dataset)\n",
    "    * [3.1 Defining A Custom PyTorch Dataset Class For Medical Image Segmentation](#3.1-Defining-A-Custom-PyTorch-Dataset-Class-For-Medical-Image-Segmentation)\n",
    "    * [3.2 Defining The Custom LightningDataModule Class](#3.2-Defining-The-Custom-LightningDataModule-Class)\n",
    "    * [3.3 Visualization Helper Functions](#3.3-Visualization-Helper-Functions)\n",
    "    * [3.4 Display Sample Images From The Dataset](#3.4-Display-Sample-Images-From-The-Dataset)\n",
    "* [4 Loading SegFormer From ü§ó HuggingFace](#4-Loading-SegFormer-From-ü§ó-HuggingFace)\n",
    "* [5 Evaluation Metric & Loss Function](#5-Evaluation-Metric--&-Loss-Function)\n",
    "    * [5.1 Custom Loss Functions - Smooth Dice + Cross-Entropy](#5.1-Custom-Loss-Functions---Smooth-Dice-+-Cross-Entropy)\n",
    "    * [5.2 Evaluation Metric - Dice Coefficient (F1-Score)](#5.2-Evaluation-Metric---Dice-Coefficient-(F1-Score))\n",
    "* [6 Creating The Custom LightningModule Class](#6-Creating-The-Custom-LightningModule-Class)\n",
    "* [7 Start Training](#7-Start-Training)\n",
    "* [8 Inference on the Medical Segmentation Dataset](#8-Inference-on-the-Medical-Segmentation-Dataset)\n",
    "    * [8.1 Load The Best Trained Model](#8.1-Load-The-Best-Trained-Model)\n",
    "    * [8.2 Evaluate Model On Validation Dataset](#8.2-Evaluate-Model-On-Validation-Dataset)\n",
    "    * [8.3 Image Inference Using DataLoader Objects](#8.3-Image-Inference-Using-DataLoader-Objects)\n",
    "* [9 Summary](#9-Summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37d4afe-14fd-46c4-893c-56d73e87ebab",
   "metadata": {},
   "source": [
    "## What is Medical Image Segmentation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4220c3-8e8b-4e2b-8406-f82fb2f4d51e",
   "metadata": {},
   "source": [
    "Medical image segmentation is a process that involves dividing medical images, such as CT scans or MRI scans, into distinct regions or structures of interest. This technique is used to identify and isolate specific areas within the image, which is crucial for diagnosis, treatment planning, and monitoring of diseases. It can be done manually by experts or automated using computer algorithms and machine learning. Medical image segmentation plays a vital role in various medical specialties and enables quantitative analysis and precise measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869048c8-1165-4a6e-bce3-fe2fb8f8a3c0",
   "metadata": {},
   "source": [
    "The dataset for this project is taken from the <a href=\"https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/overview\" target=\"_blank\">UW-Madison GI Tract Image Segmentation</a> Kaggle competition. The dataset consists of 3 classes: the stomach, small bowel, and large bowel.\n",
    "\n",
    "<img src=\"https://learnopencv.com/wp-content/uploads/2023/07/medical-image-segmentation_competition_dataset_example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eff3dec-032d-4ac9-a6b8-fdb772e0e91e",
   "metadata": {},
   "source": [
    "**Note:** In this notebook, we'll work with the final processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81c1934",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T07:54:27.130481Z",
     "start_time": "2023-07-14T07:54:25.857835Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ce8770",
   "metadata": {},
   "source": [
    "## 1 Install & Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af254f96",
   "metadata": {},
   "source": [
    "Before we begin the coding part, we need to ensure we have all the required libraries installed. For this project, apart from PyTorch, we are installing additional tools to help ease the implementation process. \n",
    "\n",
    "The major ones are:\n",
    "\n",
    "<img src=\"https://learnopencv.com/wp-content/uploads/2023/07/medical-image-segmentation_tool_logos.png\">\n",
    "\n",
    "1. `transformers`: To load SegFormer transformer model.\n",
    "2. `lightning`: To simplify and structure code implementations.\n",
    "3. `torchmetrics`: For evaluating the model's performance.\n",
    "4. `wandb`: For experiment tracking. \n",
    "5. `albumentations`:  For applying augmentations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797424c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T07:54:27.146552Z",
     "start_time": "2023-07-14T07:54:27.133026Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install libraries and restart kernel.\n",
    "%pip install -qqqU wandb transformers lightning albumentations torchmetrics torchinfo\n",
    "%pip install -qqq requests gradio\n",
    "%pip install -qqq ipywidgets chardet charset-normalizer==3.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2208608",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T07:54:27.162224Z",
     "start_time": "2023-07-14T07:54:27.146552Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import platform\n",
    "import warnings\n",
    "from glob import glob\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# To filter UserWarning.\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001c1515",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T07:54:38.775081Z",
     "start_time": "2023-07-14T07:54:27.162224Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# For data augmentation and preprocessing.\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Imports required SegFormer classes\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "# Importing lighting along with a built-in callback it provides.\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Importing torchmetrics modular and functional implementations.\n",
    "from torchmetrics import MeanMetric\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "\n",
    "# To print model summary.\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880832c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T07:54:38.807356Z",
     "start_time": "2023-07-14T07:54:38.775081Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sets the internal precision of float32 matrix multiplications.\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# To enable determinism.\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "\n",
    "# To render the matplotlib figure in the notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7085b383",
   "metadata": {},
   "source": [
    "For this project, instead of the default tensorboard used by pytorch-lightning for tracking experiments, we will use a proper MLOps tool: Weights & Biases (WandB). \n",
    "\n",
    "The following code cell will help us to log into our `wandb` account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b869cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1996c1e6",
   "metadata": {},
   "source": [
    "The code cell will ask you to paste your API key in the dialogue box. You need to click on the <a href=\"https://wandb.ai/authorize\" target=\"_blank\">Sign In with Auth0</a> link provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be812d13",
   "metadata": {},
   "source": [
    "## 2 Set Hyperparameters For The Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325ee1d2",
   "metadata": {},
   "source": [
    "Next, we will declare all the different hyperparameters used for the project. For this, we are defining three dataclasses. They will be used throughout the notebook.\n",
    "\n",
    "\n",
    "1. `DatasetConfig`  ‚Äì A class that holds all the hyperparameters we will use to process images. It contains the following information:\n",
    "    1. Image size to use.\n",
    "    2. Number of classes present in the dataset,\n",
    "    3. The mean and standard deviation to use for image normalization.\n",
    "    4. URL of the preprocessed dataset.\n",
    "    5. Directory path to download the dataset to. \n",
    "    \n",
    "2. `Paths` ‚Äì This class contains the locations of the images and masks of the train and validation sets. It uses the ‚Äúroot dataset path‚Äù  set DatasetConfig as the base.\n",
    "\n",
    "3. `TrainingConfig` ‚Äì  A class that holds all the hyperparameters we will use for training and evaluation.  It contains the following information:\n",
    "    1. Batch size.\n",
    "    2. Initial learning rate.\n",
    "    3. The number of epochs to train the model.\n",
    "    4. The number of workers to use for data loading.\n",
    "    4. Model, optimizer & learning rate scheduler-related configurations.\n",
    "\n",
    "4. `InferenceConfig` ‚Äì This class contains the (optional) batch size and the number of batches we will use to display our inference results at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c89a4b8-1f01-4536-b181-01aac74d0f92",
   "metadata": {},
   "source": [
    "Note: We‚Äôve uploaded the preprocessed dataset to our Dropbox and <a href=\"https://www.kaggle.com/datasets/learnopencvblog/uwm-gi-tract-segmentation-img-msk-split\" target=\"_blank\">Kaggle</a> accounts. There are two options. You can manually download the dataset and move it to your workstation or utilize the data download code we‚Äôve written below to do it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1683f5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-14T07:54:25.866Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DatasetConfig:\n",
    "    NUM_CLASSES:   int = 4 # including background.\n",
    "    IMAGE_SIZE: tuple[int,int] = (288, 288) # W, H\n",
    "    MEAN: tuple = (0.485, 0.456, 0.406)\n",
    "    STD:  tuple = (0.229, 0.224, 0.225)\n",
    "    BACKGROUND_CLS_ID: int = 0\n",
    "    URL: str = r\"https://www.dropbox.com/scl/fi/r0685arupp33sy31qhros/dataset_UWM_GI_Tract_train_valid.zip?rlkey=w4ga9ysfiuz8vqbbywk0rdnjw&dl=1\"\n",
    "    DATASET_PATH: str = os.path.join(os.getcwd(), \"dataset_UWM_GI_Tract_train_valid\")\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Paths:\n",
    "    DATA_TRAIN_IMAGES: str = os.path.join(DatasetConfig.DATASET_PATH, \"train\", \"images\", r\"*.png\")\n",
    "    DATA_TRAIN_LABELS: str = os.path.join(DatasetConfig.DATASET_PATH, \"train\", \"masks\",  r\"*.png\")\n",
    "    DATA_VALID_IMAGES: str = os.path.join(DatasetConfig.DATASET_PATH, \"valid\", \"images\", r\"*.png\")\n",
    "    DATA_VALID_LABELS: str = os.path.join(DatasetConfig.DATASET_PATH, \"valid\", \"masks\",  r\"*.png\")\n",
    "        \n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    BATCH_SIZE:      int = 48 # 8\n",
    "    NUM_EPOCHS:      int = 100\n",
    "    INIT_LR:       float = 3e-4\n",
    "    NUM_WORKERS:     int = 0 if platform.system() == \"Windows\" else os.cpu_count()\n",
    "\n",
    "    OPTIMIZER_NAME:  str = \"AdamW\"\n",
    "    WEIGHT_DECAY:  float = 1e-4\n",
    "    USE_SCHEDULER:  bool = True # Use learning rate scheduler?\n",
    "    SCHEDULER:       str = \"MultiStepLR\" # Name of the scheduler to use.\n",
    "    MODEL_NAME:str = \"nvidia/segformer-b4-finetuned-ade-512-512\" \n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class InferenceConfig:\n",
    "    BATCH_SIZE:  int = 10\n",
    "    NUM_BATCHES: int = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f317fea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T06:57:58.983210Z",
     "start_time": "2023-07-14T06:57:58.967096Z"
    }
   },
   "source": [
    "## 3 Loading The Medical Image Segmentation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa9af69",
   "metadata": {},
   "source": [
    "**Set class ID to RGB color mapping and vice versa.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de70d1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of class ID to RGB value.\n",
    "id2color = {\n",
    "    0: (0, 0, 0),    # background pixel\n",
    "    1: (0, 0, 255),  # Stomach\n",
    "    2: (0, 255, 0),  # Small Bowel\n",
    "    3: (255, 0, 0),  # large Bowel\n",
    "}\n",
    "\n",
    "\n",
    "DatasetConfig.NUM_CLASSES = len(id2color)\n",
    "\n",
    "print(\"Number of classes\", DatasetConfig.NUM_CLASSES)\n",
    "\n",
    "# Reverse id2color mapping.\n",
    "# Used for converting RGB mask to a single channel (grayscale) representation.\n",
    "rev_id2color = {value: key for key, value in id2color.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b32f1d",
   "metadata": {},
   "source": [
    "### 3.1 Defining A Custom PyTorch Dataset Class For Medical Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebcc96b-ebc5-47b5-aece-0a706600a72b",
   "metadata": {},
   "source": [
    "First, we will define our custom PyTorch `Dataset` class. This custom is designed to load images and masks for each image. The `Dataset` class is essential for efficient and organized data handling in machine learning tasks. It provides a standardized interface to load and preprocess data samples from various sources. Encapsulating the dataset into a single object simplifies data management. It enables seamless integration with other PyTorch components like data loaders and models.¬†\n",
    "\n",
    "The custom class performs the following functions:\n",
    "\n",
    "1. Load each image-mask pair.\n",
    "2. Apply geometric and pixel augmentations if the pair belongs to the training set.\n",
    "3. Apply preprocessing transformations such as normalization and standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e76e6b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-14T07:54:25.876Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom Class for creating training and validation (segmentation) dataset objects.\n",
    "\n",
    "class MedicalDataset(Dataset):\n",
    "    def __init__(self, *, image_paths, mask_paths, img_size, ds_mean, ds_std, is_train=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths  = mask_paths  \n",
    "        self.is_train    = is_train\n",
    "        self.img_size    = img_size\n",
    "        self.ds_mean = ds_mean\n",
    "        self.ds_std = ds_std\n",
    "        self.transforms  = self.setup_transforms(mean=self.ds_mean, std=self.ds_std)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def setup_transforms(self, *, mean, std):\n",
    "        transforms = []\n",
    "\n",
    "        # Augmentation to be applied to the training set.\n",
    "        if self.is_train:\n",
    "            transforms.extend([\n",
    "                A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5),\n",
    "                A.ShiftScaleRotate(scale_limit=0.12, rotate_limit=0.15, shift_limit=0.12, p=0.5),\n",
    "                A.RandomBrightnessContrast(p=0.5),\n",
    "                A.CoarseDropout(max_holes=8, max_height=self.img_size[1]//20, max_width=self.img_size[0]//20, min_holes=5, fill_value=0, mask_fill_value=0, p=0.5)\n",
    "            ])\n",
    "\n",
    "        # Preprocess transforms - Normalization and converting to PyTorch tensor format (HWC --> CHW).\n",
    "        transforms.extend([\n",
    "                A.Normalize(mean=mean, std=std, always_apply=True),\n",
    "                ToTensorV2(always_apply=True),  # (H, W, C) --> (C, H, W)\n",
    "        ])\n",
    "        return A.Compose(transforms)\n",
    "\n",
    "    def load_file(self, file_path, depth=0):\n",
    "        file = cv2.imread(file_path, depth)\n",
    "        if depth == cv2.IMREAD_COLOR:\n",
    "            file = file[:, :, ::-1]\n",
    "        return cv2.resize(file, (self.img_size), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load image and mask file.\n",
    "        image = self.load_file(self.image_paths[index], depth=cv2.IMREAD_COLOR)\n",
    "        mask  = self.load_file(self.mask_paths[index],  depth=cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Apply Preprocessing (+ Augmentations) transformations to image-mask pair\n",
    "        transformed = self.transforms(image=image, mask=mask)\n",
    "        image, mask = transformed[\"image\"], transformed[\"mask\"].to(torch.long)\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f200ca17",
   "metadata": {},
   "source": [
    "### 3.2 Defining The Custom LightningDataModule Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5d167f-d13c-4082-b1cd-0e7c915cae14",
   "metadata": {},
   "source": [
    "In this section, we will define the custom `MedicalSegmentationDataModule` class inherited from Lightning‚Äôs `LightningDataModule` class. It helps organize and encapsulate all the data-related operations and logic in a PyTorch project. It acts as a bridge between your data and Lightning‚Äôs training pipeline. It is a convenient abstraction that encapsulates data-related operations, promotes code organization, and facilitates seamless integration with other Lightning components for efficient and reproducible deep-learning experiments.\n",
    "\n",
    "The class will perform the following functions:\n",
    "\n",
    "1. Download the dataset from Dropbox.\n",
    "2. Create a MedicalDataset class object for each set.\n",
    "3. Create and return the DataLoader objects for each set.\n",
    "\n",
    "\n",
    "The class methods we need to define are as follows:\n",
    "\n",
    "1. `prepare_data(..)`: This method is used for data preparation, like downloading and one-time preprocessing with the dataset. When training in a distributed setting, this will be called from each GPU machine.\n",
    "2. `setup(...)`:  When you want to perform data operations on every GPU, this method is apt for it will call from every GPU. For example, perform train/val/test splits.\n",
    "3. `train_dataloader(...)`: This method returns the train dataloader.\n",
    "4. `val_dataloader(...)` : This method returns validation dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc1d17b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-14T07:54:25.880Z"
    }
   },
   "outputs": [],
   "source": [
    "class MedicalSegmentationDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=10,\n",
    "        img_size=(384, 384),\n",
    "        ds_mean=(0.485, 0.456, 0.406),\n",
    "        ds_std=(0.229, 0.224, 0.225),\n",
    "        batch_size=32,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        shuffle_validation=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.img_size    = img_size\n",
    "        self.ds_mean     = ds_mean\n",
    "        self.ds_std      = ds_std\n",
    "        self.batch_size  = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory  = pin_memory\n",
    "        \n",
    "        self.shuffle_validation = shuffle_validation\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Download dataset.\n",
    "        dataset_zip_path = f\"{DatasetConfig.DATASET_PATH}.zip\"\n",
    "\n",
    "        # Download if dataset does not exists.\n",
    "        if not os.path.exists(DatasetConfig.DATASET_PATH):\n",
    "\n",
    "            print(\"Downloading and extracting assets...\", end=\"\")\n",
    "            file = requests.get(DatasetConfig.URL)\n",
    "            open(dataset_zip_path, \"wb\").write(file.content)\n",
    "\n",
    "            try:\n",
    "                with zipfile.ZipFile(dataset_zip_path) as z:\n",
    "                    z.extractall(os.path.split(dataset_zip_path)[0]) # Unzip where downloaded.\n",
    "                    print(\"Done\")\n",
    "            except:\n",
    "                print(\"Invalid file\")\n",
    "\n",
    "            os.remove(dataset_zip_path) # Remove the ZIP file to free storage space.\n",
    "\n",
    "    def setup(self, *args, **kwargs):\n",
    "        # Create training dataset and dataloader.\n",
    "        train_imgs = sorted(glob(f\"{Paths.DATA_TRAIN_IMAGES}\"))\n",
    "        train_msks  = sorted(glob(f\"{Paths.DATA_TRAIN_LABELS}\"))\n",
    "\n",
    "        # Create validation dataset and dataloader.\n",
    "        valid_imgs = sorted(glob(f\"{Paths.DATA_VALID_IMAGES}\"))\n",
    "        valid_msks = sorted(glob(f\"{Paths.DATA_VALID_LABELS}\"))\n",
    "\n",
    "        self.train_ds = MedicalDataset(image_paths=train_imgs, mask_paths=train_msks, img_size=self.img_size,  \n",
    "                                       is_train=True, ds_mean=self.ds_mean, ds_std=self.ds_std)\n",
    "\n",
    "        self.valid_ds = MedicalDataset(image_paths=valid_imgs, mask_paths=valid_msks, img_size=self.img_size, \n",
    "                                       is_train=False, ds_mean=self.ds_mean, ds_std=self.ds_std)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # Create train dataloader object with drop_last flag set to True.\n",
    "        return DataLoader(\n",
    "            self.train_ds, batch_size=self.batch_size,  pin_memory=self.pin_memory, \n",
    "            num_workers=self.num_workers, drop_last=True, shuffle=True\n",
    "        )    \n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # Create validation dataloader object.\n",
    "        return DataLoader(\n",
    "            self.valid_ds, batch_size=self.batch_size,  pin_memory=self.pin_memory, \n",
    "            num_workers=self.num_workers, shuffle=self.shuffle_validation\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83ec9b4",
   "metadata": {},
   "source": [
    "**Usage**: Let's download the dataset and initialize train and validation data loaders. We‚Äôll use them to visualize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abb40b2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-14T07:54:25.887Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dm = MedicalSegmentationDataModule(\n",
    "    num_classes=DatasetConfig.NUM_CLASSES,\n",
    "    img_size=DatasetConfig.IMAGE_SIZE,\n",
    "    ds_mean=DatasetConfig.MEAN,\n",
    "    ds_std=DatasetConfig.STD,\n",
    "    batch_size=InferenceConfig.BATCH_SIZE,\n",
    "    num_workers=0,\n",
    "    shuffle_validation=True,\n",
    ")\n",
    "\n",
    "# Donwload dataset.\n",
    "dm.prepare_data()\n",
    "\n",
    "# Create training & validation dataset.\n",
    "dm.setup()\n",
    "\n",
    "train_loader, valid_loader = dm.train_dataloader(), dm.val_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8f84a7",
   "metadata": {},
   "source": [
    "### 3.3 Visualization Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886b0afb",
   "metadata": {},
   "source": [
    "To help visualize our dataset, we need to define some additional helper functions. They are as follows:\n",
    "\n",
    "A) `num_to_rgb(...)`: Function will be used to convert single-channel mask representations to an integrated RGB mask for visualization purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdaabfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T07:57:11.921526Z",
     "start_time": "2023-07-14T07:57:11.825989Z"
    }
   },
   "outputs": [],
   "source": [
    "def num_to_rgb(num_arr, color_map=id2color):\n",
    "    single_layer = np.squeeze(num_arr)\n",
    "    output = np.zeros(num_arr.shape[:2] + (3,))\n",
    " \n",
    "    for k in color_map.keys():\n",
    "        output[single_layer == k] = color_map[k]\n",
    " \n",
    "    # return a floating point array in range [0.0, 1.0]\n",
    "    return np.float32(output) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f56734",
   "metadata": {},
   "source": [
    "B) `image_overlay(...)`: This function overlays an RGB segmentation map on top of an RGB image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03897833",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T07:57:11.921526Z",
     "start_time": "2023-07-14T07:57:11.825989Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to overlay a segmentation map on top of an RGB image.\n",
    "def image_overlay(image, segmented_image):\n",
    "    alpha = 1.0  # Transparency for the original image.\n",
    "    beta = 0.7  # Transparency for the segmentation map.\n",
    "    gamma = 0.0  # Scalar added to each sum.\n",
    "\n",
    "    segmented_image = cv2.cvtColor(segmented_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    image = cv2.addWeighted(image, alpha, segmented_image, beta, gamma, image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return np.clip(image, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71173192",
   "metadata": {},
   "source": [
    "C) `display_image_and_mask(...)`: The convenience function below will display the original image, the ground truth mask, and the ground truth mask overlayed on the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044a8eeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T07:57:11.921526Z",
     "start_time": "2023-07-14T07:57:11.825989Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_image_and_mask(*, images, masks, color_map=id2color):\n",
    "    title = [\"GT Image\", \"Color Mask\", \"Overlayed Mask\"]\n",
    "\n",
    "    for idx in range(images.shape[0]):\n",
    "        image = images[idx]\n",
    "        grayscale_gt_mask = masks[idx]\n",
    "\n",
    "        fig = plt.figure(figsize=(15, 4))\n",
    "\n",
    "        # Create RGB segmentation map from grayscale segmentation map.\n",
    "        rgb_gt_mask = num_to_rgb(grayscale_gt_mask, color_map=color_map)\n",
    "\n",
    "        # Create the overlayed image.\n",
    "        overlayed_image = image_overlay(image, rgb_gt_mask)\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(title[0])\n",
    "        plt.imshow(image)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title(title[1])\n",
    "        plt.imshow(rgb_gt_mask)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.imshow(rgb_gt_mask)\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(title[2])\n",
    "        plt.imshow(overlayed_image)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db679bc0",
   "metadata": {},
   "source": [
    "D) `denormalize(...)`: This function is used to denormalize the image tensors and clip values between `0` and `1`. It is used to denormalize the images for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c56e4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T07:57:11.921526Z",
     "start_time": "2023-07-14T07:57:11.825989Z"
    }
   },
   "outputs": [],
   "source": [
    "def denormalize(tensors, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "    for c in range(3):\n",
    "        tensors[:, c, :, :].mul_(std[c]).add_(mean[c])\n",
    "\n",
    "    return torch.clamp(tensors, min=0.0, max=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1259998b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T07:47:58.926115Z",
     "start_time": "2023-07-14T07:47:58.893856Z"
    }
   },
   "source": [
    "### 3.4 Display Sample Images From The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ecdec0",
   "metadata": {},
   "source": [
    "In the code cell below, we loop over the first batch in the validation dataset and display the ground truth image, ground truth mask, and the ground truth mask overlayed on the image. The overlay helps us better visualize the segmented classes in the context of the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ec74f9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-14T07:54:25.893Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for batch_images, batch_masks in valid_loader:\n",
    "\n",
    "    batch_images = denormalize(batch_images, mean=DatasetConfig.MEAN, std=DatasetConfig.STD).permute(0, 2, 3, 1).numpy()\n",
    "    batch_masks  = batch_masks.numpy()\n",
    "\n",
    "    print(\"batch_images shape:\", batch_images.shape)\n",
    "    print(\"batch_masks shape: \", batch_masks.shape)\n",
    "    \n",
    "    display_image_and_mask(images=batch_images, masks=batch_masks)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9cfdc0",
   "metadata": {},
   "source": [
    "## 4 Loading SegFormer From ü§ó HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2db8851-59a3-44c5-9db3-5a37e87a48ff",
   "metadata": {},
   "source": [
    "The SegFormer model was proposed in the paper titled <a href=\"https://arxiv.org/abs/2105.15203\" target=\"_blank\">SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers.</a> The model consists of a hierarchical <a href=\"learnopencv.com/the-future-of-image-recognition-is-here-pytorch-vision-transformer/\" target=\"_blank\">Transformer</a> encoder made of efficient multi-head attention modules and a final lightweight all-MLP decoder head.\n",
    "\n",
    "Abstract from the paper:\n",
    "\n",
    "> We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C.\n",
    "\n",
    "<img src=\"https://learnopencv.com/wp-content/uploads/2023/07/medical-image-segmentation_Segformer_architecture.png\" width=\"75%\" align=\"center\">\n",
    "\n",
    "Source: Arxiv paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2d70f8-bcd3-4e41-9858-8974a3db52f4",
   "metadata": {},
   "source": [
    "**You can check all the trained weights available for SegFormer model on HuggingFace <a href=\"https://huggingface.co/models?pipeline_tag=image-segmentation&sort=downloads&search=nvidia%2Fsegformer\" target=\"_blank\">over here.</a>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c92069-cdbc-4b78-bef1-772e4a6a185a",
   "metadata": {},
   "source": [
    "Loading a pre-trained model version and getting it ready for inference or finetuning is very easy, thanks to HuggingFace. We only have to pass the following:\n",
    "\n",
    "1. `pretrained_model_name_or_path`: (string). The id/path of a pre-trained model hosted on the Huggingface model zoo.\n",
    "2. `num_labels`: (int) The number of channels (one for each class) we want the model to give as output. Suppose the number differs from the original number. In that case, the layer will be replaced with a new layer with randomly initialized weights.\n",
    "3. `ignore_mismatched_sizes`: (bool) Boolean value to whether or not to ignore the weight key mismatch. Here, it occurs because we change the `num_labels` value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fd0ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(*, model_name, num_classes):\n",
    "    model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_classes,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cc8d98-1512-4ec0-981c-4acf4a5aedc2",
   "metadata": {},
   "source": [
    "**Usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ba4816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = get_model(model_name=TrainingConfig.MODEL_NAME, num_classes=DatasetConfig.NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7ae788-b879-489f-9401-8ff129523a6d",
   "metadata": {},
   "source": [
    "* The model's forward pass takes multiple arguments <a href=\"https://huggingface.co/docs/transformers/v4.15.0/model_doc/segformer#transformers.SegformerForSemanticSegmentation.forward\" target=\"_blank\">[SegFormer Documentation]</a>. The two important ones are `pixel_values` and `labels`.\n",
    "* The `pixel_values` argument refers to the input images. The `labels` argument is for passing the ground-truth mask.  \n",
    "* The model's forward pass also calculates the cross-entropy loss if `labels` are passed.\n",
    "* The output logits are smaller than the input image size. To get the **outputs** to match the input image size, we need to simply **upsample** it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d83dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy inputs.\n",
    "print(DatasetConfig.IMAGE_SIZE[::-1])\n",
    "data    = torch.randn(1, 3, *DatasetConfig.IMAGE_SIZE[::-1])\n",
    "target = torch.rand(1, *DatasetConfig.IMAGE_SIZE[::-1]).to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03c85d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy outputs.\n",
    "outputs = model(pixel_values=data, labels=target, return_dict=True)\n",
    "\n",
    "# Upsample model outputs to match input image size.\n",
    "upsampled_logits = nn.functional.interpolate(outputs[\"logits\"], size=target.shape[-2:], mode=\"bilinear\", align_corners=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96abcda1-f763-4187-aff8-549d9e94927f",
   "metadata": {},
   "source": [
    "To access the model's output, we have to use the `[\"logits\"]` key. Similarly, we can access the loss via the `\"loss\"` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7af57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Outputs: outputs['logits']:\", outputs[\"logits\"].shape)\n",
    "\n",
    "print(\"Model Outputs Resized::\", upsampled_logits.shape)\n",
    "\n",
    "print(\"Loss: outputs['loss']:\", outputs[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61278a9d-1c96-4c83-8acf-630f17371059",
   "metadata": {},
   "source": [
    "In this project, we won‚Äôt be using the CE loss returned by the model for training. Instead, we will define our custom combo loss function that combines the Smooth Dice coefficient & CE to compute the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd6d90-f58e-4aef-a741-9e495b214b02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T15:05:59.938863Z",
     "start_time": "2023-07-19T15:05:58.940508Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary(model, input_size=(1, 3, *DatasetConfig.IMAGE_SIZE[::-1]), depth=2, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11225f5e",
   "metadata": {},
   "source": [
    "## 5 Evaluation Metric  & Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a56baa",
   "metadata": {},
   "source": [
    "The **Dice Coefficient** (otherwise known as the *F1-Score*) is a function that is commonly used in the context of segmentation and is often specifically used as the basis for a loss function for segmentation problems. We will write the custom loss function next based on the Dice Coefficient, but let's first provide the motivation for why this might be a good idea. \n",
    "\n",
    "For a binary classification problem, the metric is defined as follows using set notation, where `A` and `B` are segmentation masks representing the ground truth mask and the predicted segmentation map. \n",
    "<br>\n",
    "\n",
    "$$ \n",
    "Dice = \\frac{2*|A\\cap B\\hspace{1mm}|}{|A\\hspace{1mm}| + |B\\hspace{1mm}|} \\hspace{2mm}\n",
    "$$\n",
    "\n",
    "Simply put, the metric is twice the overlap area divided by the total number of pixels in both images. As you can see, the Dice Coefficient is very similar to IoU. Both metrics range from `0` to `1` and are positively correlated with each other. In terms of confusion matrix components, the metric can also be defined as follows:\n",
    "<br>\n",
    "$$ Dice =  \\hspace{2mm} \\frac{2TP}{2TP + FP + FN}\n",
    "$$\n",
    "<br>\n",
    "\n",
    "However, the Dice Coefficient is not quite as intuitive as IoU. To better understand the formulation, we need to consider two important quantities that lead to its development: Precision and Recall, as defined below. \n",
    "<br>\n",
    "\n",
    "$$P:= \\frac{TP}{TP + FP}  \\hspace{10mm} R:= \\frac{TP}{TP + FN}$$\n",
    "\n",
    "\n",
    "<br>\n",
    "Precision is a measure of how precise the model is in making predictions (quality or purity of the positive predictions), and Recall considers what we missed or describes the completeness of the positive predictions. \n",
    "This is the motivation that gave rise to the development of the Dice Coefficient (F1-Score) below, defined as the harmonic mean of the two quantities (a balancing between the two quantities):\n",
    "<br>\n",
    "\n",
    "$$Dice = (\\frac{2}{\\frac{1}{P} + \\frac{1}{R}}) \\hspace{2mm} =  \\hspace{2mm} \\frac{2TP}{2TP + FP + FN}$$\n",
    "<br>\n",
    "\n",
    "Another way to look at each component is by referring to the following figure from Wikipedia for the <a href=\"https://en.wikipedia.org/wiki/F-score\" target=\"_blank\">F1-Score</a>. Here we see that it's important to consider which elements are relevant and which elements are retrieved. In this context, it is easy to see that both Precision and Recall are essential components for quantifying the accuracy of a model.\n",
    "\n",
    "<img src='https://opencv.org/wp-content/uploads/2022/07/c4-05-precision-recall.png' align='center' width=\"60%\">\n",
    "\n",
    "---\n",
    "\n",
    "Note that the Dice Coefficient can also be used as an evaluation metric and is used in the Kaggle competition as an evaluation metric along with 3D Hausdorff distance. But since, for this project, we are focusing on 2D images, we will stick with the Dice coefficient as our primary evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08483fbc",
   "metadata": {},
   "source": [
    "### 5.1 Custom Loss Functions - Smooth Dice + Cross-Entropy\n",
    "\n",
    "Below, we define a custom loss function often used in segmentation problems when there is an imbalance in the classes within the dataset. The loss is based on the Dice metric and combined with Cross-entropy (CE) loss. \n",
    "\n",
    "Dice + CE is a good loss function for semantic segmentation as it combines pixel-wise accuracy with boundary alignment, encouraging precise object localization. It addresses the class imbalance issue by incorporating the Dice coefficient, promoting balanced predictions and improving overall segmentation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c0d312-5c2c-4806-9753-96ee747508df",
   "metadata": {},
   "source": [
    "In practice, we found that using a combined loss (Dice loss + CCE loss) works better than Dice loss alone. This is also supported by our experiments:\n",
    "\n",
    "<img src=\"https://learnopencv.com/wp-content/uploads/2023/07/medical-image-segmentation_run_f1_compare.png\">\n",
    "\n",
    "The gray one refers to *Dice + CE loss* & the green is for *only Dice loss*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e56bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef_loss(predictions, ground_truths, num_classes=2, dims=(1, 2), smooth=1e-8):\n",
    "    \"\"\"Smooth Dice coefficient + Cross-entropy loss function.\"\"\"\n",
    "\n",
    "    ground_truth_oh = F.one_hot(ground_truths, num_classes=num_classes)\n",
    "    prediction_norm = F.softmax(predictions, dim=1).permute(0, 2, 3, 1)\n",
    "\n",
    "    intersection = (prediction_norm * ground_truth_oh).sum(dim=dims)\n",
    "    summation = prediction_norm.sum(dim=dims) + ground_truth_oh.sum(dim=dims)\n",
    "\n",
    "    dice = (2.0 * intersection + smooth) / (summation + smooth)\n",
    "    dice_mean = dice.mean()\n",
    "\n",
    "\n",
    "    CE = F.cross_entropy(predictions, ground_truths)\n",
    "\n",
    "    return (1.0 - dice_mean) + CE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58e347e",
   "metadata": {},
   "source": [
    "### 5.2 Evaluation Metric - Dice Coefficient (F1-Score) \n",
    "\n",
    "\n",
    "To calculate the Dice score for the medical image segmentation task, we will use the `MulticlassF1Score` class from the `torchmetrics` library with the \"`macro`\" average reduction method.\n",
    "\n",
    "**Macro** average refers to a method of calculating average performance in multiclass or multilabel classification problems, which treats all classes equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b09369",
   "metadata": {},
   "source": [
    "## 6 Creating The Custom LightningModule Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b2defd-284f-4284-99c0-9027f316f5e3",
   "metadata": {},
   "source": [
    "The final custom class we need to create is the `MedicalSegmentationModel` which inherits its functionalities from Lightning‚Äôs `LightningModule` class.\n",
    "\n",
    "The `LightningModule` class in pytorch-lightning is a higher-level abstraction that simplifies the training and organizing of PyTorch models. It provides a structured, standardized interface for defining and training deep learning models. It separates the concerns of model definition, optimization, and training loop, making the code more modular and readable.\n",
    "\n",
    "\n",
    "The class methods we need to define are as follows:\n",
    "\n",
    "1. Model initialization: `__init__(...)` method where the model and its parameters are defined. This method also includes the initialization of the loss and metric calculation methods.\n",
    "2. Forward pass: `forward(...)` method where the forward pass of the model is defined.\n",
    "Training step: training_step(...) method where the training step for each batch is defined. It includes calculating loss and metrics, which are logged for tracking.\n",
    "3. Validation step: `validation_step(...)` method where the validation step for each batch is defined. It also includes the calculation of loss and metrics.\n",
    "4. Optimizer configuration: `configure_optimizers(...)` method where the optimizer and, optionally, the learning rate scheduler are defined.\n",
    "\n",
    "Moreover, two methods, `on_train_epoch_end(...)` and `on_validation_epoch_end(...)`, are defined to log the average loss and f1 score after each epoch for training and validation, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec06e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalSegmentationModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        num_classes: int = 10,\n",
    "        init_lr: float = 0.001,\n",
    "        optimizer_name: str = \"Adam\",\n",
    "        weight_decay: float = 1e-4,\n",
    "        use_scheduler: bool = False,\n",
    "        scheduler_name: str = \"multistep_lr\",\n",
    "        num_epochs: int = 100,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Save the arguments as hyperparameters.\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Loading model using the function defined above.\n",
    "        self.model = get_model(model_name=self.hparams.model_name, num_classes=self.hparams.num_classes)\n",
    "\n",
    "        # Initializing the required metric objects.\n",
    "        self.mean_train_loss = MeanMetric()\n",
    "        self.mean_train_f1 = MulticlassF1Score(num_classes=self.hparams.num_classes, average=\"macro\")\n",
    "        self.mean_valid_loss = MeanMetric()\n",
    "        self.mean_valid_f1 = MulticlassF1Score(num_classes=self.hparams.num_classes, average=\"macro\")\n",
    "\n",
    "    def forward(self, data):\n",
    "        outputs = self.model(pixel_values=data, return_dict=True)\n",
    "        upsampled_logits = F.interpolate(outputs[\"logits\"], size=data.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        return upsampled_logits\n",
    "    \n",
    "    def training_step(self, batch, *args, **kwargs):\n",
    "        data, target = batch\n",
    "        logits = self(data)\n",
    "\n",
    "        # Calculate Combo loss (Segmentation specific loss (Dice) + cross entropy)\n",
    "        loss = dice_coef_loss(logits, target, num_classes=self.hparams.num_classes)\n",
    "        \n",
    "        self.mean_train_loss(loss, weight=data.shape[0])\n",
    "        self.mean_train_f1(logits.detach(), target)\n",
    "\n",
    "        self.log(\"train/batch_loss\", self.mean_train_loss, prog_bar=True, logger=False)\n",
    "        self.log(\"train/batch_f1\", self.mean_train_f1, prog_bar=True, logger=False)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        # Computing and logging the training mean loss & mean f1.\n",
    "        self.log(\"train/loss\", self.mean_train_loss, prog_bar=True)\n",
    "        self.log(\"train/f1\", self.mean_train_f1, prog_bar=True)\n",
    "        self.log(\"epoch\", self.current_epoch)\n",
    "\n",
    "    def validation_step(self, batch, *args, **kwargs):\n",
    "        data, target = batch\n",
    "        logits = self(data)\n",
    "        \n",
    "        # Calculate Combo loss (Segmentation specific loss (Dice) + cross entropy)\n",
    "        loss = dice_coef_loss(logits, target, num_classes=self.hparams.num_classes)\n",
    "\n",
    "        self.mean_valid_loss.update(loss, weight=data.shape[0])\n",
    "        self.mean_valid_f1.update(logits, target)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \n",
    "        # Computing and logging the validation mean loss & mean f1.\n",
    "        self.log(\"valid/loss\", self.mean_valid_loss, prog_bar=True)\n",
    "        self.log(\"valid/f1\", self.mean_valid_f1, prog_bar=True)\n",
    "        self.log(\"epoch\", self.current_epoch)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = getattr(torch.optim, self.hparams.optimizer_name)(\n",
    "            filter(lambda p: p.requires_grad, self.model.parameters()),\n",
    "            lr=self.hparams.init_lr,\n",
    "            weight_decay=self.hparams.weight_decay,\n",
    "        )\n",
    "\n",
    "        LR = self.hparams.init_lr\n",
    "        WD = self.hparams.weight_decay\n",
    "\n",
    "        if self.hparams.optimizer_name in (\"AdamW\", \"Adam\"):\n",
    "            optimizer = getattr(torch.optim, self.hparams.optimizer_name)(model.parameters(), lr=LR, \n",
    "                                                                          weight_decay=WD, amsgrad=True)\n",
    "        else:\n",
    "            optimizer = optim.SGD(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "        if self.hparams.use_scheduler:\n",
    "            lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[self.trainer.max_epochs // 2,], gamma=0.1)\n",
    "\n",
    "            # The lr_scheduler_config is a dictionary that contains the scheduler\n",
    "            # and its associated configuration.\n",
    "            lr_scheduler_config = {\"scheduler\": lr_scheduler, \"interval\": \"epoch\", \"name\": \"multi_step_lr\"}\n",
    "            return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}\n",
    "\n",
    "        else:\n",
    "            return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcfd962",
   "metadata": {},
   "source": [
    "## 7 Start Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0192e22-77cf-460e-9199-0315b30975b2",
   "metadata": {},
   "source": [
    "Once we have organized the `LightningModule` and `LightningDataModule` classes, we can utilize Lightning's `Trainer` class to automate the remaining tasks effortlessly.\n",
    "\n",
    "The `Trainer` offers a range of valuable deep-learning training functionalities, such as mixed-precision training, distributed training, deterministic training, profiling, gradient accumulation, batch overfitting, and more. Implementing these functionalities correctly can be time-consuming, but it becomes a swift process with the `Trainer` class.\n",
    "\n",
    "By initializing our `MedicalSegmentationModel` and `MedicalSegmentationDataModule` classes and passing them to the `.fit(...)` method of the `Trainer` class instance, we can promptly commence training. This streamlined approach eliminates the need to implement various training aspects manually, providing convenience and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dacc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed everything for reproducibility.\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "model = MedicalSegmentationModel(\n",
    "    model_name=TrainingConfig.MODEL_NAME,\n",
    "    num_classes=DatasetConfig.NUM_CLASSES,\n",
    "    init_lr=TrainingConfig.INIT_LR,\n",
    "    optimizer_name=TrainingConfig.OPTIMIZER_NAME,\n",
    "    weight_decay=TrainingConfig.WEIGHT_DECAY,\n",
    "    use_scheduler=TrainingConfig.USE_SCHEDULER,\n",
    "    scheduler_name=TrainingConfig.SCHEDULER,\n",
    "    num_epochs=TrainingConfig.NUM_EPOCHS,\n",
    ")\n",
    "\n",
    "data_module = MedicalSegmentationDataModule(\n",
    "    num_classes=DatasetConfig.NUM_CLASSES,\n",
    "    img_size=DatasetConfig.IMAGE_SIZE,\n",
    "    ds_mean=DatasetConfig.MEAN,\n",
    "    ds_std=DatasetConfig.STD,\n",
    "    batch_size=TrainingConfig.BATCH_SIZE,\n",
    "    num_workers=TrainingConfig.NUM_WORKERS,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9902c544",
   "metadata": {},
   "source": [
    "Next, we will define a `ModelCheckpoint` and a `LearningRateMonitor` callback for saving the best model during training and the current learning rate of an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a3c646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating ModelCheckpoint callback. \n",
    "# We'll save the model on basis on validation f1-score.\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    monitor=\"valid/f1\",\n",
    "    mode=\"max\",\n",
    "    filename=\"ckpt_{epoch:03d}-vloss_{valid/loss:.4f}_vf1_{valid/f1:.4f}\",\n",
    "    auto_insert_metric_name=False,\n",
    ")\n",
    "\n",
    "# Creating a learning rate monitor callback which will be plotted/added in the default logger.\n",
    "lr_rate_monitor = LearningRateMonitor(logging_interval=\"epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d8588a",
   "metadata": {},
   "source": [
    "We will also initialize the `WandbLogger` to upload the training metrics to your wandb project.\n",
    " \n",
    "During the logger initialization, we set two parameters:\n",
    "1. `log_model=True` - Upload the model as an artifact when the training is completed.\n",
    "2. `project` - The project name to use on WandB. A project typically contains logs from multiple experiments and their checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86555cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logger.\n",
    "wandb_logger = WandbLogger(log_model=True, project=\"UM_medical_segmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ded1bfb",
   "metadata": {},
   "source": [
    "When the logger is initialized, it will also print the link for the current experiment, which you open on any device to monitor the training process and also share with your team."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493a90ea",
   "metadata": {},
   "source": [
    "**Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7c4c2a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initializing the Trainer class object.\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",  # Auto select the best hardware accelerator available\n",
    "    devices=\"auto\",  # Auto select available devices for the accelerator (For eg. mutiple GPUs)\n",
    "    strategy=\"auto\",  # Auto select the distributed training strategy.\n",
    "    max_epochs=TrainingConfig.NUM_EPOCHS,  # Maximum number of epoch to train for.\n",
    "    enable_model_summary=False,  # Disable printing of model summary as we are using torchinfo.\n",
    "    callbacks=[model_checkpoint, lr_rate_monitor],  # Declaring callbacks to use.\n",
    "    precision=\"16-mixed\",  # Using Mixed Precision training.\n",
    "    logger=wandb_logger\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e3ca6f-82d8-422a-b8d3-d4f7c1db3832",
   "metadata": {},
   "source": [
    "## 8 Inference on the Medical Segmentation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bad6d7",
   "metadata": {},
   "source": [
    "For inference, we will use the same validation data as we did during training. We will plot the ground truth images, the ground truth masks, and the predicted segmentation maps overlayed on the ground truth images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae184d6",
   "metadata": {},
   "source": [
    "### 8.1 Load The Best Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c99fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path of the best saved model.\n",
    "CKPT_PATH = model_checkpoint.best_model_path\n",
    "CKPT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c321c2",
   "metadata": {},
   "source": [
    "Initialize the model with trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11705a57-12ba-43db-97c4-1983acb4cffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MedicalSegmentationModel.load_from_checkpoint(CKPT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c1506",
   "metadata": {},
   "source": [
    "### 8.2 Evaluate Model On Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec744601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the validation dataloader.\n",
    "\n",
    "data_module.setup()\n",
    "valid_loader = data_module.val_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2419ad30",
   "metadata": {},
   "source": [
    "Get the best evaluation metrics using the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56da51d9-7021-41e3-9b91-9194f3102254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer class for inference.\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,        \n",
    "    enable_checkpointing=False,\n",
    "    inference_mode=True,\n",
    ")\n",
    "\n",
    "# Run evaluation.\n",
    "results = trainer.validate(model=model, dataloaders=valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11739183-98d8-4105-b65d-a53be2074f45",
   "metadata": {},
   "source": [
    "<img src=\"https://learnopencv.com/wp-content/uploads/2023/07/medical-image-segmentation_best_run_results_chart.png\">\n",
    "\n",
    "*Blue* - Train, *Orange* - Valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5793b7f0",
   "metadata": {},
   "source": [
    "Log them as experiment summary metrics to WandB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264d7265-47c4-4ad5-988e-0bf6f40fd981",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get(\"LOCAL_RANK\", None) is None:\n",
    "    wandb.run.summary[\"best_valid_f1\"] = results[0][\"valid/f1\"]\n",
    "    wandb.run.summary[\"best_valid_loss\"] = results[0][\"valid/loss\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedc5ee6-10c4-4978-918b-daddce6d0fc5",
   "metadata": {},
   "source": [
    "### 8.3 Image Inference Using DataLoader Objects\n",
    "\n",
    "In the code below, we define a helper function  that performs `inference` given a trained model and a dataloader object. The model prediction will also be uploaded to wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264e090d-cdb4-431a-9dc1-26833f43d951",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def inference(model, loader, img_size, device=\"cpu\"):\n",
    "    num_batches_to_process = InferenceConfig.NUM_BATCHES\n",
    "\n",
    "    for idx, (batch_img, batch_mask) in enumerate(loader):\n",
    "        predictions = model(batch_img.to(device))\n",
    "\n",
    "        pred_all = predictions.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        batch_img = denormalize(batch_img.cpu(), mean=DatasetConfig.MEAN, std=DatasetConfig.STD)\n",
    "        batch_img = batch_img.permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "        if idx == num_batches_to_process:\n",
    "            break\n",
    "\n",
    "        for i in range(0, len(batch_img)):\n",
    "            fig = plt.figure(figsize=(20, 8))\n",
    "\n",
    "            # Display the original image.\n",
    "            ax1 = fig.add_subplot(1, 4, 1)\n",
    "            ax1.imshow(batch_img[i])\n",
    "            ax1.title.set_text(\"Actual frame\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            # Display the ground truth mask.\n",
    "            true_mask_rgb = num_to_rgb(batch_mask[i], color_map=id2color)\n",
    "            ax2 = fig.add_subplot(1, 4, 2)\n",
    "            ax2.set_title(\"Ground truth labels\")\n",
    "            ax2.imshow(true_mask_rgb)\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            # Display the predicted segmentation mask.\n",
    "            pred_mask_rgb = num_to_rgb(pred_all[i], color_map=id2color)\n",
    "            ax3 = fig.add_subplot(1, 4, 3)\n",
    "            ax3.set_title(\"Predicted labels\")\n",
    "            ax3.imshow(pred_mask_rgb)\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            # Display the predicted segmentation mask overlayed on the original image.\n",
    "            overlayed_image = image_overlay(batch_img[i], pred_mask_rgb)\n",
    "            ax4 = fig.add_subplot(1, 4, 4)\n",
    "            ax4.set_title(\"Overlayed image\")\n",
    "            ax4.imshow(overlayed_image)\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "            \n",
    "            # Upload predictions to WandB.\n",
    "            images = wandb.Image(fig, caption=f\"Prediction Sample {idx}_{i}\")\n",
    "            \n",
    "            if os.environ.get(\"LOCAL_RANK\", None) is None:\n",
    "                wandb.log({\"Predictions\": images})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bc4110-a11b-40fc-bf4b-4584fa30e7c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use GPU if available.\n",
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "inference(model, valid_loader, device=DEVICE, img_size=DatasetConfig.IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c90117",
   "metadata": {},
   "source": [
    "Terminate the wandb experiment run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5e3cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get(\"LOCAL_RANK\", None) is None:\n",
    "    wandb.run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424b0514-7e5d-4cff-aa12-66df036d5b74",
   "metadata": {},
   "source": [
    "You can use the Gradio inference demo app here --> <a href=\"https://huggingface.co/spaces/veb-101/UWMGI_Medical_Image_Segmentation\" target=\"_blank\">Medical Image Segmentation Gradio App</a>\n",
    "\n",
    "All the files required can be accessed from here --> <a href=\"https://huggingface.co/spaces/veb-101/UWMGI_Medical_Image_Segmentation/tree/main\" target=\"_blank\">Gradio App Files</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d4aebf",
   "metadata": {},
   "source": [
    "## 9 Summary\n",
    "\n",
    "Medical image segmentation using deep learning provides significant advantages. Deep learning models excel at capturing complex patterns and features, leading to highly accurate and precise segmentation results compared to traditional methods. Additionally, deep learning algorithms automate segmentation, improving efficiency and enabling analysis of large volumes of medical image data. Moreover, deep learning models demonstrate adaptability and generalization, making them suitable for diverse image characteristics, imaging modalities, patient populations, and clinical settings, expanding their utility in medical imaging applications.\n",
    "\n",
    "To summarise this articleüìú, we covered a comprehensive list of related topics:\n",
    "\n",
    "1. Medical Image Segmentation: Explored the definition and challenges of medical image segmentation.\n",
    "2. Dataset Preparation: Used the UW-Madison GI Tract segmentation dataset, made observations, and created preprocessed training and validation sets.\n",
    "3. We defined a few essential functions and classes for PyTorch and PyTorch-Lightning frameworks to facilitate ease of training.\n",
    "4. We learned how to use the Segformer model from Hugging Face transformers for segmentation and fine-tuned it on our dataset.\n",
    "5. We defined a custom loss function combining the Dice coefficient with cross-entropy for improved segmentation performance.\n",
    "6. Training and Metrics Tracking: Trained the model, monitored metrics using WandB, and uploaded the model as an artifact for future use.\n",
    "7. We designed a user-friendly interface using the Gradio app, making our medical multi-label image classification model accessible to everyone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "299.896px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
